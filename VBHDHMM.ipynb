{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dacb6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from Data import *\n",
    "from utils import *\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "plt.style.use(\"classic\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7775ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_experimental_indicators(all_params, true_params):\n",
    "    mse = torch.mean((all_params - true_params) ** 2)\n",
    "    mae = torch.mean(torch.abs(all_params - true_params))\n",
    "    bias = torch.mean((all_params.mean(0) - true_params) ** 2)\n",
    "    mse_sd = torch.std((all_params - true_params) ** 2)\n",
    "    mae_sd = torch.std(torch.abs(all_params - true_params))\n",
    "    bias_sd = torch.std((all_params.mean(0) - true_params) ** 2)\n",
    "    \n",
    "    res = {\n",
    "        \"MSE\": mse.item(),\n",
    "        \"MSE_SD\": mse_sd.item(),\n",
    "        \"MAE\": mae.item(),\n",
    "        \"MAE_SD\": mae_sd.item(),\n",
    "        \"Bias\": bias.item(),\n",
    "        \"Bias_SD\": bias_sd.item()\n",
    "    }\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "class VBHMMs:\n",
    "    def __init__(self, Y, X, K,beta0,Z,bei=3.0):\n",
    "        self.Y = Y\n",
    "        self.X = X\n",
    "        self.N = X.shape[0]  \n",
    "        self.K = K\n",
    "        self.d = X.shape[1]\n",
    "        self.alpha_pi = 1 / self.K * torch.ones([K])\n",
    "        self.alpha_A = torch.ones([K])\n",
    "        self.mu_0 = torch.zeros([self.d, 1])\n",
    "        self.Sigma0 = torch.eye(self.d)\n",
    "        self.p_k = 1 / K * torch.ones([self.N, self.K])\n",
    "        self.Sigma = 1 * torch.eye(self.d)\n",
    "        self.pi_p = 1 / self.K * torch.ones([self.K])\n",
    "        self.A_p = 1 / self.K * torch.ones([self.K, self.K])\n",
    "        self.X_p = torch.softmax(torch.nn.functional.one_hot(Z.long().reshape(-1),K)*bei,dim=1)\n",
    "        self.mu = beta0.clone()+torch.distributions.Normal(0,0.1).sample([K,beta.shape[1]])\n",
    "        self.sigma = torch.eye(1).unsqueeze(0).repeat([K, self.d, self.d])\n",
    "        self.mu_1 = torch.ones([K, self.d])\n",
    "        self.lambd_1 = torch.ones([K, self.d])\n",
    "        r = 1.0  \n",
    "        delta = 1.0  \n",
    "        gamma_dist = torch.distributions.gamma.Gamma(r, 1.0 / delta)\n",
    "        self.lambd = gamma_dist.sample([K, self.d])\n",
    "    def update_pi(self):\n",
    "        p1 = self.X_p[0]\n",
    "        alpha_pi_star = p1 + self.alpha_pi\n",
    "        self.pi_p = alpha_pi_star\n",
    "    def update_A(self):\n",
    "        for j in range(self.K):\n",
    "            p_n_1 = self.X_p[:-1, j].view(-1, 1)\n",
    "            p_n = self.X_p[1:, :]\n",
    "            alpha_A_j_star = torch.sum(p_n_1 * p_n, 0) + self.alpha_A\n",
    "            self.A_p[j] = alpha_A_j_star\n",
    "    def update_beta(self):\n",
    "        for k in range(self.K):\n",
    "            ig = stats.invgauss(self.mu_1.numpy(), self.lambd_1.numpy())\n",
    "            term1 = 0\n",
    "            term2 = 0\n",
    "            for n in range(self.N):\n",
    "                x_n = self.X[n].view(-1, 1)\n",
    "                y_n = self.Y[n]\n",
    "                term1 += 1 / sigma ** 2 * self.X_p[n, k] * x_n.mm(x_n.T)\n",
    "                term2 += 1 / sigma ** 2 * self.X_p[n, k] * y_n * x_n.T\n",
    "            self.sigma[k] = torch.inverse(\n",
    "                term1 + 1 / sigma ** 2 * torch.diag(torch.from_numpy(ig.mean()))[k] + torch.eye(self.d)*0.0001\n",
    "            )\n",
    "            self.mu[k] = term2.mm(self.sigma[k]).view(-1)\n",
    "\n",
    "    def update_Z(self, num_smaple=10000):\n",
    "        pi_dist = torch.distributions.Dirichlet(self.pi_p)\n",
    "        pi_samples = pi_dist.sample([num_smaple])\n",
    "        E_log_pi = torch.mean(torch.log(pi_samples), 0)\n",
    "        A_dist = torch.distributions.Dirichlet(self.A_p)\n",
    "        A_samples = A_dist.sample([num_smaple])\n",
    "        E_log_A = torch.mean(torch.log(A_samples), 0)\n",
    "        y1 = self.Y[0]\n",
    "        x1 = self.X[0]\n",
    "        beta = np.ones((num_smaple,3,self.d))\n",
    "        for j in range(num_smaple):\n",
    "            for i in range(self.K):\n",
    "                beta[j,i,:] = np.random.multivariate_normal(self.mu[i,:],self.sigma[i,:,:])\n",
    "        beta = torch.tensor(beta,dtype=torch.float32)\n",
    "        torch.bmm(beta,x1.view(-1, 1).repeat([num_smaple, 1, 1]))\n",
    "        normal_y1 = torch.distributions.Normal(\n",
    "            beta.bmm(x1.view(-1, 1).repeat([num_smaple, 1, 1])), sigma\n",
    "        )\n",
    "        E_log_p_y1 = normal_y1.log_prob(y1).mean(0).view(-1)\n",
    "        p1 = E_log_pi + E_log_p_y1 + (E_log_A * self.X_p[1]).sum(1)\n",
    "        self.X_p[0] = torch.distributions.Multinomial(1, logits=p1).mean\n",
    "\n",
    "        for n in range(1, self.N - 1):\n",
    "            yn = self.Y[n]\n",
    "            xn = self.X[n]\n",
    "\n",
    "            normal_yn = torch.distributions.Normal(\n",
    "                beta.bmm(xn.view(-1, 1).repeat([num_smaple, 1, 1])), sigma\n",
    "            )\n",
    "            E_log_p_yn = normal_yn.log_prob(yn).mean(0).view(-1)\n",
    "          \n",
    "            pn = (\n",
    "                torch.sum(E_log_A.T * (self.X_p[n - 1]) + E_log_A * self.X_p[n + 1], 1)\n",
    "                + E_log_p_yn\n",
    "            )\n",
    "            \n",
    "            self.X_p[n] = torch.distributions.Multinomial(1, logits=pn).mean\n",
    "\n",
    "       \n",
    "        yN = self.Y[0]\n",
    "        xN = self.X[0]\n",
    "        \n",
    "        normal_yN = torch.distributions.Normal(\n",
    "            beta.bmm(xN.view(-1, 1).repeat([num_smaple, 1, 1])), sigma\n",
    "        )\n",
    "        E_log_p_yN = normal_yN.log_prob(yN).mean(0).view(-1)\n",
    "        pN = torch.sum(E_log_A * (self.X_p[self.N - 1].T), 0) + E_log_p_yN\n",
    "        self.X_p[self.N - 1] = torch.distributions.Multinomial(1, logits=pN).mean\n",
    "\n",
    "    def update_xi(self):\n",
    "        for k in range(self.K):\n",
    "            for j in range(self.d):\n",
    "                self.mu_1[k, j] = (\n",
    "                    self.lambd[j, k]\n",
    "                    * sigma ** 2\n",
    "                    / (self.mu[k, j] ** 2 + self.sigma[k, j, j])\n",
    "                )\n",
    "        self.lambd_1 = self.lambd\n",
    "\n",
    "    def update(self, iters=1):\n",
    "        self.mus = torch.zeros([iters, self.K, self.d])\n",
    "        for i in range(iters):\n",
    "            self.update_pi()\n",
    "            self.update_A()\n",
    "            self.update_beta()\n",
    "            self.update_Z()\n",
    "            self.mus[i] = self.mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c5085b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BP\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "#linear\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeacfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生模拟数据\n",
    "sigma = 0.4\n",
    "K = 3\n",
    "p =30\n",
    "beta = torch.zeros([K, p])\n",
    "\n",
    "beta[0, :4] = torch.tensor([0.5, -2, 2, -1])\n",
    "beta[1, :4] = torch.tensor([1, -2, 1.5, -1.5])\n",
    "beta[2, :4] = torch.tensor([1.5, -1.5, 1, -2])\n",
    "\n",
    "N = 300\n",
    "# 初始概率\n",
    "pi = torch.tensor([0.6, 0.3, 0.1])\n",
    "# 转移概率论矩阵\n",
    "A = torch.tensor([[0.2, 0.3, 0.5], [0.1, 0.6, 0.3], [0.5, 0.4, 0.1]])\n",
    "batch_size=8\n",
    "T=100\n",
    "all_beta=torch.zeros([T,K,p])\n",
    "all_prob=torch.zeros([T,K,K])\n",
    "all_model=[]\n",
    "n_test = 10\n",
    "num_index=4\n",
    "VBHMMs_pred=torch.zeros([T,n_test])\n",
    "LSTM_pred=torch.zeros([T,n_test])\n",
    "BP_pred=torch.zeros([T,n_test])\n",
    "LM_pred=torch.zeros([T,n_test])\n",
    "pred_result={\"VBHMMs\":torch.zeros([T,num_index]),\n",
    "             \"LSTM\":torch.zeros([T,num_index]),\n",
    "             \"BP\":torch.zeros([T,num_index]),\n",
    "             \"LM\":torch.zeros([T,num_index]),\n",
    "             }\n",
    "\n",
    "for t in range(T):\n",
    "\n",
    "    torch.random.manual_seed(t)\n",
    "    X = torch.distributions.Normal(0, 2).sample([N, p])\n",
    "    Y, Z = generate_data(X, beta, sigma, pi, A, N)\n",
    "\n",
    "\n",
    "    X_train = X[:-n_test]\n",
    "    X_test = X[-n_test:]\n",
    "    y_train = Y[:-n_test]\n",
    "    y_test = Y[-n_test:]\n",
    "    z_train=Z[:-n_test]\n",
    "    z_test = Z[-n_test:]\n",
    "\n",
    "\n",
    "    torch.random.manual_seed(t)\n",
    "    model = VBHMMs(y_train, X_train, K,beta,z_train,5.)\n",
    "\n",
    "    mu_0=model.mu\n",
    "    mu_1=model.mu+1\n",
    "    MAX_iter = 100\n",
    "    cnt=0\n",
    "    while torch.mean((mu_0-mu_1)**2)>1e-6 and cnt<MAX_iter:\n",
    "        mu_0=model.mu.clone()\n",
    "        model.update(1)\n",
    "        model.mu=model.mu[model.mu[:,0].argsort()]\n",
    "        mu_1=model.mu.clone()\n",
    "        cnt+=1\n",
    "    Dir = torch.distributions.Dirichlet(model.A_p)\n",
    "    transition_probs = Dir.mean\n",
    "    all_beta[t]=model.mu\n",
    "    all_prob[t]=transition_probs\n",
    "    all_model.append(model)\n",
    "\n",
    "    all_beta[t]=model.mu\n",
    "    all_prob[t]=transition_probs\n",
    "    all_model.append(model)\n",
    "    #VBHMMs\n",
    "    labels_train = model.X_p.argmax(1)\n",
    "    Dir = torch.distributions.Dirichlet(model.A_p)\n",
    "    transition_probs = Dir.mean.numpy()\n",
    "\n",
    "    y_test_pred = predict_obs(\n",
    "        model, labels_train[-1].item(), transition_probs, X_test, n_test, 1\n",
    "    )\n",
    "    VBHMMs_pred[t]=y_test_pred\n",
    "\n",
    "    #LSTM\n",
    "    data_set=TensorDataset(X_train.unsqueeze(1),y_train)\n",
    "    data_laoder=DataLoader(data_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "    lr=1e-3\n",
    "    num_epoch=50\n",
    "    hidden_size=10\n",
    "    setup_seed(t)\n",
    "    lstm_model = LSTM(p,hidden_size,False)\n",
    "    loss_list=lstm_model.train_step(data_laoder,lr,num_epoch)\n",
    "    lstm_pred=lstm_model(X_test).detach()\n",
    "    LSTM_pred[t]=lstm_pred.view(-1)\n",
    "\n",
    "    #BP\n",
    "    bp=MLPRegressor(hidden_layer_sizes=[20],random_state=0,max_iter=int(1e4))\n",
    "    bp.fit(X_train,y_train)\n",
    "    bp_pred=torch.from_numpy(bp.predict(X_test)).float()\n",
    "    BP_pred[t]=bp_pred\n",
    "\n",
    "    #LinearRegression\n",
    "    lm=RandomForestRegressor(n_estimators=100,random_state=0)\n",
    "    lm.fit(X_train,y_train)\n",
    "    lm_pred=torch.from_numpy(lm.predict(X_test)).float()\n",
    "    LM_pred[t]=lm_pred.view(-1)\n",
    "\n",
    "    #result\n",
    "    pred_result[\"VBHMMs\"][t]=torch.FloatTensor(list(evaluate_model(y_test,VBHMMs_pred[t]).values()))\n",
    "    pred_result[\"LSTM\"][t]=torch.FloatTensor(list(evaluate_model(y_test,LSTM_pred[t]).values()))\n",
    "    pred_result[\"BP\"][t]=torch.FloatTensor(list(evaluate_model(y_test,BP_pred[t]).values()))\n",
    "    pred_result[\"LM\"][t]=torch.FloatTensor(list(evaluate_model(y_test,LM_pred[t]).values()))\n",
    "\n",
    "    print(\"{}/{} iter = {} finished!\".format(t+1,T,cnt))\n",
    "\n",
    "# 原来的代码\n",
    "# pred_result[\"VBHMMs\"]=dict(zip(['MAPE', 'RMSE', 'MAE', 'R2'],pred_result[\"VBHMMs\"].mean(0).tolist()))\n",
    "# pred_result[\"LSTM\"]=dict(zip(['MAPE', 'RMSE', 'MAE', 'R2'],pred_result[\"LSTM\"].mean(0).tolist()))\n",
    "# pred_result[\"BP\"]=dict(zip(['MAPE', 'RMSE', 'MAE', 'R2'],pred_result[\"BP\"].mean(0).tolist()))\n",
    "# pred_result[\"LM\"]=dict(zip(['MAPE', 'RMSE', 'MAE', 'R2'],pred_result[\"LM\"].mean(0).tolist()))\n",
    "\n",
    "# 修改后的代码\n",
    "metrics = ['MAPE', 'RMSE', 'MAE', 'R2']\n",
    "for method in ['VBHMMs', 'LSTM', 'BP', 'LM']:\n",
    "    means = pred_result[method].mean(0)\n",
    "    stds = pred_result[method].std(0)\n",
    "    pred_result[method] = {metric: {'mean': mean.item(), 'std': std.item()} for metric, mean, std in zip(metrics, means, stds)}\n",
    "\n",
    "# 打印结果\n",
    "for key, value in pred_result.items():\n",
    "    print(key, value)\n",
    "\n",
    "print(\"beta:\")\n",
    "print(repeated_experimental_indicators(all_beta,beta))\n",
    "print(\"A\")\n",
    "print(repeated_experimental_indicators(all_prob,A))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
